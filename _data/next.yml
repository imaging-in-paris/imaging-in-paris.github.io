
- speaker: Pascal Monasse
  date: February 4th
  time: 2pm
  affiliation: IMAGINE, École Nationale des Ponts et Chaussées
  url: https://imagine.enpc.fr/~monasse/
  title: "Tree representations of images and applications"
  abstract: "Starting from the Reeb tree to component trees and variants, various proposals have been made for abstracting the level sets of images and their relationship. The simplest one is obtained by representing an image as a continuous function through interpolation. The easiest way to achieve it is through bilinear interpolation of the samples. Its tree of level lines can be efficiently recovered and the defects of bilinear interpolation on the regularity of level lines can be fixed through affine shortening. Several applications from mean curvature estimation to vectorization are presented. Curiously, the discrete representation of the image by a simplicial complex imposes more restrictions on what can be done. Provided they are fulfilled, several fast algorithms compete to compute the tree in this case."
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)

- speaker: Flavier Léger
  date: February 4th
  time: 3pm
  affiliation: INRIA, Cérémade, Université Paris Dauphine
  url: https://flavienleger.github.io/
  title: "Gradient descent with a general cost"
  abstract: "In this talk I will present an approach to iteratively minimize a given objective function using minimizing movement schemes built on general cost functions. I will introduce an explicit method, gradient descent with a general cost (GDGC), as well as an implicit, proximal-like scheme and an explicit-implicit (forward-backward) method. <br/>
GDGC unifies several standard gradient descent-type methods: gradient descent, mirror descent, Newton’s method, and Riemannian gradient descent. I will explain how the so-called nonnegative cross-curvature condition provides tractable conditions to prove convergence rates for GDGC. <br/>
Byproducts of this framework include: (1) a new nonsmooth mirror descent, (2) global convergence rates for Newton’s method, and (3) a clear picture of the type of convexity needed for converging schemes in the Riemannian setting. "
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)


- speaker: Matthieu Serfaty
  date: March 4th
  time: 2pm
  affiliation: Centre Borelli, ENS Paris-Saclay
  url: 
  title: ""
  abstract: ""
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)


- speaker: Yanhao Li
  date: March 4th
  time: 3pm
  affiliation: Centre Borelli, ENS Paris-Saclay
  url: 
  title: ""
  abstract: ""
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)


- speaker: Clément Rambour
  date: April 1st
  time: 2pm
  affiliation: ISIR, Sorbonne Université
  url: https://clementrambour.github.io/
  title: ""
  abstract: ""
  room: Amphi Hermite (Bat Borel)

- speaker: Pietro Gori
  date: April 1st
  time: 3pm
  affiliation: Télécom Paris
  url: https://perso.telecom-paristech.fr/pgori/index.html
  title: "Contrastive Learning in Computer Vision and Medical Imaging - A metric learning approach"
  abstract: "Contrastive Learning (CL) is a paradigm designed for self-supervised representation learning which has been applied to unsupervised, weakly supervised and supervised problems. The objective in CL is to estimate a parametric mapping function that maps positive samples (semantically similar) close together in the representation space and negative samples (semantically dissimilar) far away from each other. In general, positive samples can be defined in different ways depending on the problem: transformations (i.e., augmentations) of the same image (unsupervised setting), samples belonging to the same class (supervised) or with similar image attributes (weakly-supervised). The definition of negative samples varies accordingly. In this talk, we will show how a metric learning approach for CL allows us to: 1- better formalize recent contrastive losses, such as InfoNCE and SupCon, 2- derive new losses for unsupervised, supervised, and weakly supervised problems, and 3- propose new regularization terms for debiasing. Furthermore, leveraging the proposed metric learning approach and kernel theory, we will describe a novel loss, called decoupled uniformity, that allows the integration of prior knowledge, given either by generative models or weak attributes, and removes the positive-negative coupling problem, as in the InfoNCE loss. We validate the usefulness of the proposed losses on standard vision datasets and medical imaging data."
  room: Amphi Hermite (Bat Borel)


- speaker: TBA
  date: May 6th
  time: 2pm
  affiliation: TBA
  url: 
  title: ""
  abstract: ""
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)

- speaker: Gabriel Peyré
  date: June 3rd
  time: 2pm
  affiliation: DMA, École Normale Supérieure 
  url: https://www.gpeyre.com/
  title: ""
  abstract: ""
  room: Amphi Yvonne Choquet-Bruhat (Bat Perrin)
  


