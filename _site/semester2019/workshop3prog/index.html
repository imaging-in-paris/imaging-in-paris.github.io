<!DOCTYPE html>
<html lang="en">

<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Séminaire Parisien des Mathématiques de l'Imagerie">


    <title>Imaging and machine learning - Séminaire Parisien des Mathématiques de l'Imagerie</title>

    <link rel="canonical" href="http://localhost:4000/semester2019/workshop3prog/">

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="/img/favicon.png">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <script src="/js/jquery.min.js "></script>

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Séminaire Parisien des Mathématiques de l'Imagerie" />

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

        <!-- Loading mathjax -->
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

</head>


<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Home</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">



                  <!--<li>
                    <a href="/next">Prochains</a>
                </li>
                <li>
                    <a href="/past">Passés</a>
                </li>  -->

                  <li>
                        <a href="/seminar/">Seminar</a>
                  </li>
                  <li>
                        <a href="/coming">Coming</a>
                  </li>

                <!--

                
                <li>
                    <a href="/coming/">Comming</a>
                </li>
                
                <li>
                    <a href="/index-old/">Imaging in Paris</a>
                </li>
                
                <li>
                    <a href="/seminar/">Imaging in Paris Seminar</a>
                </li>
                
                <li>
                    <a href="/">The Mathematics of Imaging</a>
                </li>
                
                <li>
                    <a href="/seminar/next/">Prochains séminaires</a>
                </li>
                
                <li>
                    <a href="/seminar/past/">Past seminar</a>
                </li>
                
                <li>
                    <a href="/semester2019/school/">The mathematics of imaging: the CIRM pre-school</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop1/">Variational methods and optimization in imaging</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop1prog/">Variational methods and optimization in imaging</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop2/">Statistical modeling for shapes and imaging</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop2prog/">Statistical modeling for shapes and imaging</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop3/">Imaging and machine learning</a>
                </li>
                
                <li>
                    <a href="/semester2019/workshop3prog/">Imaging and machine learning</a>
                </li>
                
                <li>
                    <a href="/semester2019/young/">Young researchers Imaging Seminars</a>
                </li>
                
                <li>
                    
                </li>
                
                <li>
                    
                </li>
                
                -->
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Page Header -->
<header class="intro-header" style="background-image: url('/../img/paris4.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="site-heading">
                    <h1>Imaging and machine learning</h1>
                    <hr class="small">
                    <span class="subheading">Mathematics of Imaging Workshop #3</span>
                </div>
            </div>
        </div>
    </div>
</header>



<!-- Main Content -->
<div class="container">
	<div class="row">
		<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
			<h2 id="tentative-program">Tentative program</h2>

<p>TBC</p>

<h2 id="abstracts">Abstracts</h2>

<p>
  Stéphanie Allassonnière<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Chloé-Agathe Azencott<br />
  <b>Title:</b> <i>Using structure to select features in high dimension</i><br />
  <b>Abstract:</b> Many problems in genomics require the ability to identify relevant features in data sets containing many more orders of magnitude than samples. This setup poses different statistical and computational challenges, and traditional feature selection methods fall short. In my talk, I will present several ways to incorporate prior knowledge of the structure of the features to address this problem.
</p>

<p>
  Francis Bach<br />
  <b>Title:</b> <i>On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</i><br />
  <b>Abstract:</b> Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension. (Joint work with Lénaïc Chizat)
</p>

<p>
  Erik Bekkers<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Marta Betcke<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Christoph Brune<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Jianfeng Cai<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Matthieu Cord<br />
  <b>Title:</b> <i>Designing multimodal deep architectures for Visual Question Answering</i><br />
  <b>Abstract:</b> Multimodal representation learning for text and image has been extensively studied in recent years. Currently, one of the most popular tasks in this field is Visual Question Answering (VQA). I will introduce this complex multimodal task, which aims at answering a question about an image. To solve this problem, visual and textual deep nets models are required and, high level interactions between these two modalities have to be carefully designed into the model in order to provide the right answer. This projection from the unimodal spaces to a multimodal one is supposed to extract and model the relevant correlations between the two spaces. Besides, the model must have the ability to understand the full scene, focus its attention on the relevant visual regions and discard the useless information regarding the question.
</p>

<p>
  Romain Couillet<br />
  <b>Title:</b> <i>Random Matrix Advances in Machine Learning</i><br />
  <b>Abstract:</b> Machine learning algorithms, starting from elementary yet popular ones, are difficult to theoretically analyze as (i) they are data-driven, and (ii) they rely on non-linear tools (kernels, activation functions). These theoretical limitations are exacerbated in large dimensional datasets where standard algorithms behave quite differently than predicted, if not completely fail. In this talk, we will show how random matrix theory (RMT) answers all these problems. We will precisely show that RMT provides a new understanding and various directions of improvements for kernel methods, semi-supervised learning, SVMs, community detection on graphs, spectral clustering, etc. Besides, we will show that RMT can explain observations made on real complex datasets in as advanced methods as deep neural networks.
</p>

<p>
  Daniel Cremers<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Marco Cuturi<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Cédric Févotte<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Alexandre Gramfort<br />
  <b>Title:</b> <i>Optimization meets machine learning for neuroimaging</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Hervé Jegou<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Rodolphe Jenatton<br />
  <b>Title:</b> <i>Scalable hyperparameter transfer learning</i><br />
  <b>Abstract:</b> Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization, such as hyperparameter optimization. Typically, BO relies on conventional Gaussian process (GP) regression, whose algorithmic complexity is cubic in the number of evaluations. As a result, GP-based BO cannot leverage large numbers of past function evaluations, for example, to warm-start related BO runs. We propose a multi-task adaptive Bayesian linear regression model for transfer learning in BO, whose complexity is linear in the function evaluations: one Bayesian linear regression model is associated to each black-box function optimization problem (or task), while transfer learning is achieved by coupling the models through a shared neural network. A first set of experiments show that the neural network learns a representation suitable for warm-starting the black-box optimization problems and that BO runs can be accelerated when the target black-box function (e.g., validation loss) is learned together with other related signals (e.g., training loss). The proposed method was found to be at least one order of magnitude faster that methods recently published in the literature. A second set of experiments show that our approach can further be combined with Hyperband, replacing the uniform random sampling of hyperparameter candidates by an adaptive non-uniform sampling procedure. Our extension not only improves the precision resolution of Hyperband but also supports transfer learning, both, within a Hyperband run and across previous hyperparameter tuning tasks. This is joint work with V. Perrone, L. Valkov, F. Winkelmolen, C. Archambeau and M. Seeger.
</p>

<p>
  Julien Mairal<br />
  <b>Title:</b> <i>A Kernel Perspective for Regularizing Deep Neural Networks</i><br />
  <b>Abstract:</b> We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models. This is a joint work with Alberto Bietti, Gregoire Mialon and Dexiong Chen.
</p>

<p>
  Stéphane Mallat<br />
  <b>Title:</b> <i>Autoencoder Image Generation with Multiscale Sparse Deconvolutions</i><br />
  <b>Abstract:</b> Autoencoders and GAN's can synthesize remarkably complex images, although we still do not understand the mathematical properties of the generated random processes. We introduces a mathematical and algorithmic framework to analyze the principles of such image syntheses. In Wasserstein autoencoders, the coder is trained to transform the input random vector into a lower-dimensional nearly white noise. Images are synthesized from white noise with an inverse deep convolutional generator. We show that the encoder can be computed with a multiscale scattering transform, which mixes input variables at multiple scales. We prove that generating an image model then amounts to solve a sequence of linear deconvolutions at different scales. A deep convolutional generator regularizes this deconvolution by sparsity in dictionaries learned at each scale. Numerical image synthesis will be shown. Joint work with Tomas Anglès.
</p>

<p>
  Naila Murray<br />
  <b>Title:</b> <i>Predicting aesthetic appreciation of images</i><br />
  <b>Abstract:</b> Image aesthetics has become an important criterion for visual content curation on social media sites and media content repositories. Previous work on aesthetic prediction models in the computer vision community has focused on aesthetic score prediction or binary image labeling. However, raw aesthetic annotations are in the form of score histograms and provide richer and more precise information than binary labels or mean scores. In this talk I will present recent work at Naver Labs Europe on the rarely-studied problem of predicting aesthetic score distributions. The talk will cover the large-scale dataset we collected for this problem, called AVA, and will describe the novel deep architecture and training procedure for our score distribution model. Our model achieves state-of-the-art results on AVA for three tasks: (i) aesthetic quality classification; (ii) aesthetic score regression; and (iii) aesthetic score distribution prediction, all while using one model trained only for the distribution prediction task. I will also discuss our proposed method for modifying an image such that its predicted aesthetics changes, and describe how this modification can be used to gain insight into our model.
</p>

<p>
  Guillaume Obozinski<br />
  <b>Title:</b> <i>Convex unmixing and learning the effect of latent variables in Gaussian Graphical models with unobserved</i><br />
  <b>Abstract:</b> The edge structure of the graph defining an undirected graphical model describes precisely the structure of dependence between the variables in the graph. In many applications, the dependence structure is unknown and it is desirable to learn it from data, often because it is a preliminary step to be able to ascertain causal effects. This problem, known as structure learning, is hard in general, but for Gaussian graphical models it is slightly easier because the structure of the graph is given by the sparsity pattern of the precision matrix of the joint distribution, and because independence coincides with decorrelation. A major difficulty too often ignored in structure learning is the fact that if some variables are not observed, the marginal dependence graph over the observed variables will possibly be significantly more complex and no longer reflect the direct dependencies that are potentially associated with causal effects. In this work, we consider a family of latent variable Gaussian graphical models in which the graph of the joint distribution between observed and unobserved variables is sparse, and the unobserved variables are conditionally independent given the others. Prior work was able to recover the connectivity between observed variables, but could only identify the subspace spanned by unobserved variables, whereas we propose a convex optimization formulation based on structured matrix sparsity to estimate the complete connectivity of the complete graph including unobserved variables, given the knowledge of the number of missing variables, and a priori knowledge of their level of connectivity. Our formulation is supported by a theoretical result of identifiability of the latent dependence structure for sparse graphs in the infinite data limit, which is a particular instance of a more general result we prove for unmixing with convex norms. We propose an algorithm leveraging recent active set methods, which performs well in the experiments on synthetic data.
</p>

<p>
  Ozan Öktem<br />
  <b>Title:</b> <i>Bayesian inversion for tomography through machine learning</i><br />
  <b>Abstract:</b> The talk will outline recent approaches for using (deep) convolutional neural networks to solve a wide range of inverse problems, such as tomographic image reconstruction. Emphasis is on learned iterative schemes that use a neural network architecture for reconstruction that includes physics based models for how data is generated. The talk will also discuss recent developments in using generative adversarial networks for uncertainty quantification in inverse problems.
</p>

<p>
  Patrick Pérez<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Lorenzo Rosasco<br />
  <b>Title:</b> <i>Optimal machine learning with stochastic projections and regularization</i><br />
  <b>Abstract:</b> Projecting data in low dimensions is often key to scale machine learning to large high-dimensional data-sets. In this talk we will take take a statistical learning tour of classic as well as recent projection methods: from classical principal component analysis, to sketching and random subsampling. We will show that, perhaps surprisingly, there are number of settings, where it is possible to substantially reduce data dimensions, hence computational costs, without losing statistical accuracy. As a byproduct we derive a massively scalable kernel/Gaussian process solver with optimal statistical guarantees, and excellent performance in a number of large scale problems.
</p>

<p>
  Guillermo Sapiro<br />
  <b>Title:</b> <i>Learning Representations for Information Obfuscation and Inference</i><br />
  <b>Abstract:</b> Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a facial image to unlock his/her phone, or inciden-tal, such as trafﬁc cameras collecting videos on pedestrians. An undesirable side effect of these processes is that shared data can carry informa-tion about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design procedures that minimize sensitive information leak-age. Balancing the competing objectives of providing meaningful individualized service levels and inference while obfuscating sensitive information is still an open problem. In this work, we take an information theoretic approach that is implemented as an unconstrained adversarial game between Deep Neural Networks in a principled, data-driven manner. This approach enables us to learn domain-preserving stochastic transformations that maintain performance on existing algorithms while minimizing sensitive information leakage. Joint work with M. Bertran, N. Martinez, Q. Qiu, A. Papadaki, G. Reeves, and M. Rodrigues.
</p>

<p>
  Mahdi Soltanolkotabi<br />
  <b>Title:</b> <i>Towards demystifying over-parameterization in deep learning</i><br />
  <b>Abstract:</b> Many modern learning models including deep neural networks are trained in an over-parameterized regime where the parameters of the model exceed the size of the training dataset. Training these models involve highly non-convex landscapes and it is not clear how methods such as (stochastic) gradient descent provably find globally optimal models. Furthermore, due to their over-parameterized nature these neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this high fitting capacity, somewhat paradoxically, neural networks models trained via first-order methods continue to predict well on yet unseen test data. In this talk I will discuss some results aimed at demystifying such phenomena in deep learning and other domains such as matrix factorization by demonstrating that gradient methods enjoy a few intriguing properties: (1) when initialized at random the iterates converge at a geometric rate to a global optima, (2) among all global optima of the loss the iterates converge to one with a near minimal distance to the initial estimate and do so by taking a nearly direct route, (3) are provably robust to noise/corruption/shuffling on a fraction of the labels with these algorithms only fitting to the correct labels and ignoring the corrupted labels. (This talk is based on joint work with Samet Oymak)
</p>

<p>
  Jared Tanner<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Dorina Thanou<br />
  <b>Title:</b> <i>Learning graphs from data: A signal representation perspective</i><br />
  <b>Abstract:</b> The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, however, it is desirable to infer or learn a graph topology from the data. In this talk, I will survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. I will further emphasize the conceptual similarities and differences between classical and GSP-based graph inference methods, and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. Finally, I will focus on a few applications of graph learning and in particular in image coding, and conclude with several open issues and challenges that are keys to the design of future algorithms for learning graphs from data.
</p>

<p>
  Bertrand Thirion<br />
  <b>Title:</b> <i>Statistical inference in high-dimension and application to medical imaging</i><br />
  <b>Abstract:</b> Medical imaging involves high-dimensional data, yet their acquisition is obtained for limited samples. Multivariate predictive models have become popular in the last decades to fit some external variables from imaging data, and standard algorithms yield point estimates of the model parameters. It is however challenging to attribute confidence to these parameter estimates, which makes solutions hardly trustworthy. In this talk, I will present a new algorithm that assesses parameters statistical significance and that can scale even when the number of predictors p ≥ 10^5 is much higher than the number of samples n ≤ 10^3 , by leveraging structure among features. Our algorithm combines three main ingredients: a powerful inference procedure for linear models –the so-called Desparsified Lasso– feature clustering and an ensembling step. We first establish that Desparsified Lasso alone cannot handle n &lt;&lt; p regimes; then we demonstrate that the combination of clustering and ensembling provides an accurate solution, whose specificity is controlled. We also demonstrate stability improvements on two brain imaging datasets.
</p>

<p>
  Silvia Villa<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Jean-Philippe Vert<br />
  <b>Title:</b> <i>Learning from permutations</i><br />
  <b>Abstract:</b> Changes in image quality or illumination may affect the pixel intensities, without affecting the relative intensities, i.e., the ranking of pixels in an image by decreasing intensity. In order to learn a model robust to such changes, it is therefore of interest to develop machine learning tools to learn from permutations. In this talk I will discuss several approaches to embed the set of permutations to vector spaces allowing computationally efficient learning of linear models, and relate these embeddings to the classical representations of the symmetric group.
</p>

<p>
  Claire Vernade<br />
  <b>Title:</b> <i>TBA</i><br />
  <b>Abstract:</b> TBA
</p>

<p>
  Christian Wolf<br />
  <b>Title:</b> <i>Learning high-level reasoning in and from images</i><br />
  <b>Abstract:</b> Humans are able to infer what happened in a video given only a few sample frames. This faculty is called reasoning and is a key component of human intelligence. A detailed understanding requires reasoning over semantic structures, determining which objects were involved in interactions, of what nature, and what were the results of these. To compound problems, the semantic structure of a scene may change and evolve. In this talk we present research in high-level reasoning from images and videos, with the goals of understanding visual content (scene comprehension) or to make predictions of probable future outcomes, or to act in simulated environments based on visual observations. We present neural models addressing these goals through explicit modeling of object relationships; We learn this models from data or from interactions between an agent and an environment.
</p>

<h2 id="sponsors">Sponsors</h2>

<p align="center">

<a href="http://www.ihp.fr">
<img width="120" src="../../img/logo-ihp.jpg" />
</a>&nbsp;&nbsp;

<a href="http://www.cnrs.fr/">
<img width="120" src="../../img/logo-cnrs.png" />
</a>&nbsp;&nbsp;

<a href="http://www.u-psud.fr/fr/index.html">
<img width="150" src="../../img/logo-paris-sud.png" />
</a>

<br />

<a href="https://www.sciencesmaths-paris.fr/">
<img width="150" src="../../img/logo-fsmp.png" />
</a>&nbsp;&nbsp;

<a href="http://www.upmc.fr/">
<img width="150" src="../../img/logo-upmc.png" />
</a>&nbsp;&nbsp;

<a href="https://www.cimpa.info/">
<img width="150" src="../../img/logo-cimpa.png" />
</a>

<br />

<a href="http://gdr-mia.math.cnrs.fr/">
<img width="150" src="../../img/logo-mia.png" />
</a>

<a href="http://www.gpeyre.com/noria/">
<img width="150" src="../../img/logo-erc.jpg" />
</a>


</p>

		</div>
	</div>
</div>

<hr>

    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="https://github.com/imaging-in-paris">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <!--
                  <p class="copyright text-muted">Copyright &copy; Séminaire Parisien des Mathématiques de l'Imagerie 2019</p>
                -->
            </div>
        </div>
    </div>
</footer>

<!-- jQuery
<script src="/js/jquery.min.js "></script>
-->

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>




<!-- Google analytics -->
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-781488-2";
urchinTracker();
</script>


</body>

</html>
