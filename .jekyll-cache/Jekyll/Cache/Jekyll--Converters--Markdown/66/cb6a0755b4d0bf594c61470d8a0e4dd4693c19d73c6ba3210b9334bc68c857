I"“.<p>List of past seminars:</p>

<p>
  Jun 15th 2023, 15h-16h, <a href="https://tobias-liaudat.github.io/">Tobias Liadat</a> (University College London)<br />
  <b>Title:</b> <i>Recent advances in the data-driven point spread function modelling for optical telescopes</i>
  
  <br />
  <b>Abstract:</b> In astronomy, telescopes with wide-field optical instruments have a spatially varying point spread function (PSF). Certain scientific studies, like weak gravitational lensing, require a high-fidelity estimation of the PSF at target positions where no direct measurement of the PSF is provided. Even though observations of the PSF are available at some positions of the field of view, they are noisy, integrated into wavelength in the instrument's passband, and can be undersampled. PSF modelling represents a challenging ill-posed problem, as it requires building a model from these degraded observations. In this presentation, I will start by addressing recent advances for ground-based telescopes that include building the PSF model at the entire field of view at once. This problem accounts for handling discontinuities in the PSF field spatial variations, which arise from CCD-specific variations. The proposed PSF model is based on a constrained matrix factorisation framework which relies on an alternate optimisation scheme. I continue the presentation by introducing a novel framework for PSF modelling that targets space-based telescopes and, more specifically, the Euclid space mission. I propose a paradigm shift in the data-driven modelling of the instrumental response fields of telescopes. We change the data-driven modelling space from the pixels to the wavefront by adding a differentiable optical forward model into the modelling framework. This change allows transferring a great deal of complexity from the instrumental response into the forward model while adapting to the observations and remaining data-driven. Our framework allows us to build powerful physically motivated models that do not require special calibration data. We successfully model chromatic variations of the instrument's response only using noisy wide-band in-focus observations. The presentation concludes with a new optimisation procedure for the previous PSF model, where we tackle the phase retrieval problem with a model-based automatic differentiation approach. Preliminary results show that we can recover the wavefront at every position in the field of view from a set of in-focus observations. 
  </p>

<p>
  Jun 15th 2023, 14h-15h, <a href="">Mateus Sangalli</a> (Mines Paris PSL)<br />
  <b>Title:</b> <i>Equivariant neural networks based on moving frames</i>
  
  <br />
  <b>Abstract:</b> Moving frames are a classical method of obtaining invariants to the action of a Lie group on a Manifold. We apply the method of moving frames to obtain equivariant or invariant neural network layers. We show two methods to obtain equivariant networks using moving frames; one uses differential invariants as their main layer and; the other method uses a moving frame computed from the input image. We implement networks invariant to rotations in 2 and 3 dimensions and the methods are shown to have a better performance than a CNN on tasks where rotational invariance is important. The 3D rotation invariant networks are shown to increase performance on low-resolution datasets and to be more data efficient in a protein structure classification task. 
  </p>

<p>
  13 April 2023, 14h-15h, <a href="https://sites.google.com/site/marianneclausel">Marianne Clausel</a> (Universit√© de Lorraine)<br />
  <b>Title:</b> <i>Polarimetric Fourier Phase Retrieval</i>
  
  <br />
  <b>Abstract:</b> This work introduces polarimetric Fourier phase retrieval (PPR), a physically-inspired model to leverage polarization of light information in Fourier phase retrieval problems. We provide a complete characterization of its uniqueness properties by unraveling equivalencies with two related problems, namely bivariate phase retrieval and a polynomial autocorrelation factorization problem. In particular, we show that the problem admits a unique solution, which can be formulated as a greatest common divisor (GCD) of measurements polynomials. As a result, we propose algebraic solutions for PPR based on approximate GCD computations using the null-space properties Sylvester matrices. Alternatively, existing iterative algorithms for phase retrieval, semidefinite positive relaxation and Wirtinger-Flow, are carefully adapted to solve the PPR problem. Finally, a set of numerical experiments permits a detailed assessment of the numerical behavior and relative performances of each proposed reconstruction strategy. They further demonstrate the fruitful combination of algebraic and iterative approaches towards a scalable, computationally efficient and robust to noise reconstruction strategy for PPR. 
  </p>

<p>
  13 April 2023, 15h-16h, <a href="https://www.math.u-bordeaux.fr/~aleclaire/">Arthur Leclaire</a> (Institut de Math√©matiques de Bordeaux, Universit√© de Bordeaux)<br />
  <b>Title:</b> <i>Mathematical analysis of Plug and Play methods for inverse problems in imaging</i>
  
  <br />
  <b>Abstract:</b> Plug-and-Play (PnP) methods constitute a class of iterative algorithms for imaging problems where regularization is performed by a off-the-shelf denoiser. Specifically, given an image dataset, optimizing a function (e.g. a neural network) to remove Gaussian noise is equivalent to approximating the gradient or the proximal operator of the log prior of the training dataset. Therefore, any off-the-shelf denoiser can be used as an implicit prior and inserted into an optimization scheme to restore images. But the resulting PnP scheme may not directly correspond to the minimization of an explicit functional, and its convergence is thus not straightforward. In this talk, we will present several approaches that were proposed to study the convergence of such a PnP algorithm, relying on tools from non-convex optimization and fixed point theory. In particular, we will see that it is possible to learn a denoiser that writes as a gradient-step on an explicit functional, which leads to a PnP algorithm with precise numerical control in addition to state-of-the-art image restoration performance. 
  </p>

<p>
  09 February 2022, 14h-15h, <a href="https://perso.telecom-paristech.fr/nicherel/">Nicolas Cherel</a> (T√©l√©com Paris)<br />
  <b>Title:</b> <i>Patch and attention for image editing</i>
  
  <br />
  <b>Abstract:</b> We show through two different examples that patch-based methods remain relevant despite the widespread use of neural networks for many image editing tasks. We first present a patch-based algorithm for single image generation that performs as well as a neural network without requiring a costly training phase. We ensure visual fidelity and diversity of the results by carefully choosing the initialization of the algorithm. In the second part, we show that patch-based algorithms can benefit to modern techniques such as attention mechanisms. The use of attention has helped deep learning introduce long range dependencies but computing the full attention matrix is an expensive step with heavy memory and computational loads. We propose an efficient attention layer based on the stochastic algorithm PatchMatch, which is used for determining approximate nearest neighbors. Our layer has a greatly reduced memory complexity compared to other attention layers, scaling to high resolution images. 
  </p>

<p>
  09 February 2022, 15h-16h, <a href="https://www.idpoisson.fr/galerne/">Bruno Galerne</a> (Institut Denis Poisson)<br />
  <b>Title:</b> <i>Scaling Painting Style Transfer</i>
  
  <br />
  <b>Abstract:</b> Neural style transfer is a deep learning technique that produces an unprecedentedly rich style transfer from a style image to a content image and is particularly impressive when it comes to transferring style from a painting to an image. It was originally achieved by solving an optimization problem to match the global style statistics of the style image while preserving the local geometric features of the content image. The two main drawbacks of this original approach is that it is computationally expensive and that the resolution of the output images is limited by high GPU memory requirements. Many solutions have been proposed to both accelerate neural style transfer and increase its resolution, but they all compromise the quality of the produced images. Indeed, transferring the style of a painting is a complex task involving features at different scales, from the colour palette and compositional style to the fine brushstrokes and texture of the canvas. This paper provides a solution to solve the original global optimization for ultra-high resolution images, enabling multiscale style transfer at unprecedented image sizes. This is achieved by spatially localizing the computation of each forward and backward passes through the VGG network. Extensive qualitative and quantitative comparisons show that our method produces a style transfer of unmatched quality for such high resolution painting styles. 
  </p>

<p>
  15 December 2022, 14h-15h, <a href="https://sites.google.com/site/emrebaspinarhomepage/home?pli=1">Emre Baspinar</a> (CNRS-NeuroPSI, Laboratory of Computational Neuroscience)<br />
  <b>Title:</b> <i>A sub-Riemannian cortical model with frequency-phase and its application to image processing</i>
  
  <br />
  <b>Abstract:</b> In this talk, we will see a new geometrical model of the primary visual cortex together with its application to image enhancement and completion. Our departure point is the visual cortex model of the orientation selective cortical neurons, which was presented in [1]. We spatially extend this model to a five dimensional sub-Riemannian geometry and provide a novel geometric framework of the mammalian visual cortex which models orientation-frequency selective, phase shifted cortical cell behavior and the associated neural connectivity. The model extracts orientation, spatial frequency and phase information of the objects in any given two dimensional input image. Such information provides a characterization of the object boundaries and textures in the input image. We provide an image enhancement algorithm based on multi-frequency Laplace-Beltrami flow in the sub-Riemannian framework of the model. This algorithm can be modified so as to be used for image completion as well. 
  </p>

<p>
  15 December 2022, 15h-16h, <a href="https://dprn.github.io/">Dario Prandi</a> (Universit√© Paris Saclay, Centrale-Sup√©lec)<br />
  <b>Title:</b> <i>Reproducing sensory induced visual hallucinations via neural fields</i>
  
  <br />
  <b>Abstract:</b> Understanding the interaction between retinal stimulation and the cortical response in the primary visual cortex (V1 for short) is a significant challenge in improving our insight into human perception and visual organisation. In this talk we will present recent work on the reproduction of various visual illusions via continuous neural field models. In particular, we will present recent results in collaboration with Y. Chitour and C. Tamekue on the modelling via Wilson-Cowan equations of MacKay-type effects (i.e., phantom images induced by geometric patterns), showing that while the classical MacKay effect (Nature, 1957) can be recovered via a linear model, the experiences of Billock and Tsou (PNAS, 2007) are fundamentally due to the presence of a non-linearity 
  </p>

<p>
  13 Octobre 2022, 14h-15h, <a href="https://jonathanvacher.github.io/">Jonathan Vacher</a> (MAP5, Universit√© Paris-Cit√©)<br />
  <b>Title:</b> <i>Measuring uncertainty in human visual segmentation</i>
  
  <br />
  <b>Abstract:</b> Segmenting visual inputs into distinct groups of features and visual objects is central to visual function. Traditional psychophysics uncovered many rules of human perceptual segmentation, and progress in machine learning produced successful algorithms. Yet, the computational logic of human segmentation remains unclear, because we lack well-controlled paradigms to measure perceptual segmentation maps and compare models quantitatively. Here we propose a new, integrated approach: given an image, we measure multiple same--different judgments and perform model--based reconstruction of the underlying segmentation map. The reconstruction is robust to several experimental manipulations and captures the variability of individual participants. We demonstrate the approach on human segmentation of natural images and composite textures, and we show that image uncertainty affects measured human variability as well as how participants weigh different visual features. Because any segmentation algorithm can be plugged in to perform the reconstruction, our paradigm affords quantitative tests of theories of perception as well as new benchmarks for segmentation algorithms. 
  </p>

<p>
  13 Octobre 2022, 15h-16h, <a href="https://www.lip6.fr/actualite/personnes-fiche.php?ident=P1486">Isabelle Bloch</a> (LIP6 - Sorbonne Universit√©)<br />
  <b>Title:</b> <i>Hybrid AI for knowledge representation and model-based medical image understanding - Towards explainability</i>
  
  <br />
  <b>Abstract:</b> This presentation will focus on hybrid AI, as a step towards explainability, more specifically in the domain of spatial reasoning and image understanding. Image understanding benefits from the modeling of knowledge about both the scene observed and the objects it contains as well as their relationships. We show in this context the contribution of hybrid artificial intelligence, combining different types of formalisms and methods, and combining knowledge with data. Knowledge representation may rely on symbolic and qualitative approaches, as well as semi-qualitative ones to account for their imprecision or vagueness. Structural information can be modeled in several formalisms, such as graphs, ontologies, logical knowledge bases, or neural networks, on which reasoning will be based. Image understanding is then expressed as a problem of spatial reasoning. These approaches will be illustrated with examples in medical imaging, illustrating the usefulness of combining several approaches. 
  </p>

<p>
  Feb 6th, 2020, 14h-15h, <a href="https://sites.google.com/site/alasdairnewson/">Alasdair Newson</a> (T√©l√©com Paris)<br />
  <b>Title:</b> <i>Understanding and organising the latent space of autoencoders</i>
  
    <b>[<a href="../slides/newson.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Autoencoders are neural networks which project data to and from a lower dimensional latent space, the projection being learned via training on the data. While these networks produce impressive results, there is as yet little understanding of the internal mechanisms which allow autoencoders to produce such results. The work presented here has two goals. First of all, we aim to understand how the autoencoder encodes and decodes simple geometric attributes (size and position) in a very simple setting (images of disks, or simple impulses). Secondly, we present an algorithm whose goal is to organise the latent space of an autoencoder in a manner similar to Principal Component Analysis (PCA), such that each component of the latent space be statistically independent and organised in an decreasing order of importance, with respect to the $\ell^2$ norm of the reconstruction error. We refer to this autoencoder as a PCA-autoencoder. We discuss an extension of this approach to Generative Adversarial Networks. Finally we show experimental results both in controlled settings with geometrical shapes, as well as on more complex data such as the faces of Celeb-a, where our algorithm is able to discover high-level characteristics such as hair colour, smile etc. without any access to the labels of these characteristics. 
  </p>

<p>
  Feb 6th, 2020, 15h-16h, <a href="http://www.cmap.polytechnique.fr/~antonin/">Antonin Chambolle</a> (CMAP, Ecole Polytechnique)<br />
  <b>Title:</b> <i>Some remarks on the discretization of the total variation</i>
  
  <br />
  <b>Abstract:</b>  
  </p>

<p>
  7 november 2019, 14h-15h, <a href="https://rbardenet.github.io/">R√©mi Bardenet</a> (CNRS, Cristal)<br />
  <b>Title:</b> <i>DPPs everywhere: repulsive point processes for Monte Carlo integration, signal processing and machine learning</i>
  
  <br />
  <b>Abstract:</b> Determinantal point processes (DPPs) are specific repulsive point processes, which were introduced in the 1970s by Macchi to model fermion beams in quantum optics. More recently, they have been studied as models and sampling tools by statisticians and machine learners. Important statistical quantities associated to DPPs have geometric and algebraic interpretations, which makes them a fun object to study and a powerful algorithmic building block. After a quick introduction to determinantal point processes, I will discuss some of our recent statistical applications of DPPs. First, we used DPPs to sample nodes in numerical integration, resulting in Monte Carlo integration with fast convergence with respect to the number of integrand evaluations. Second, we turned DPPs into low-error variable selection procedures in linear regression. If time allows it, I'll describe a third application where we used DPP machinery to characterize the distribution of the zeros of time-frequency transforms of white noise, a recent challenge in signal processing. Joint with Ayoub Belhadji, Pierre Chainais, Julien Flamant, Guillaume Gautier, Adrien Hardy, Michal Valko. 
  </p>

<p>
  7 november 2019, 15h-16h, <a href="https://perso.univ-st-etienne.fr/deniloic/">Lo√Øc Denis</a> (Laboratoire Hubert Curien UMR 5516 CNRS / Universit√© de Saint-Etienne)<br />
  <b>Title:</b> <i>Exoplanet detection by direct imaging: a data-processing method based on patch covariances</i>
  
  <br />
  <b>Abstract:</b> The search for exoplanets is a very active subject in astronomy. Direct observation of exoplanets requires the combination of a large telescope, an extreme adaptive-optics system, a coronagraph, and dedicated data processing methods. This presentation will discuss the importance of a good statistical model of the data and describe an approach to account for the non-stationary spatial correlations in the background signal. Compared to existing approaches, this model more closely describes the data and thus leads to improved sensitivity, more accurate photometric and astrometric characterizations and more reliable results (in particular, a controlled probability of false alarms). Results obtained with the SPHERE instrument operated by the ESO at the Very Large Telescope in Chile confirm the performance of the method. An extension of the model to microscopy will also be presented. This is a joint work with Olivier Flasseur (Laboratoire Hubert Curien, CNRS/Univ St Etienne/IOGS), Eric Thi√©baut and Maud Langlois (Centre de Recherche en Astrophysique de Lyon, CNRS/Univ Lyon 1/ENS Lyon). 
  </p>

<p>
  3 october 2019, 14h-15h, <a href="https://www.irit.fr/~Emmanuel.Soubies/">Emmanuel Soubies</a> (CNRS, IRIT)<br />
  <b>Title:</b> <i>Relationships between necessary optimality conditions for the l2-l0 minimization problem.</i>
  
    <b>[<a href="../slides/soubies.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> In this talk, we will discuss the relationships between necessary optimality conditions for the l0-regularized least-squares minimization problem. Such conditions are the roots of the plethora of algorithms that have been designed to cope with this NP-hard problem. Indeed, as global optimality is in general intractable, these algorithms only ensure the convergence to suboptimal points that verify some necessary (not sufficient) optimality conditions. The degree of restrictiveness of these conditions is thus directly related to the performance of the algorithms. Within this context, we will first review the commonly used necessary optimality conditions as well as known relationships between them. Then, we will complete this hierarchy of conditions by proving new inclusion properties between the sets of candidate solutions associated to them. Moreover, we will provide a quantitative analysis of these sets. Finally, we will present numerical experiments that illustrates the fact that the performance of an algorithm is related to the restrictiveness of the optimality condition verified by the point it converges to. Joint work with Laure Blanc-F√©raud and Gilles Aubert. 
  </p>

<p>
  3 october 2019, 15h-16h, <a href="https://sites.google.com/view/yvainqueau">Yvain Queau</a> (CNRS, GREYC)<br />
  <b>Title:</b> <i>Variational methods for photometric 3D-reconstruction</i>
  
    <b>[<a href="../slides/queau.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Shape-from-shading (SfS) is a classic inverse problem consisting in reconstructing a 3D-shape from a single photography. Yet, it is an ill-posed problem, and its numerical solving is challenging. This talk will discuss the benefits of variational methods for modeling SfS, and for solving its ambiguities through the introduction of natural Bayesian priors. Efficient splitting-based algorithms will be presented, which yield state-of-the-art results on various real-world applications such as depth super-resolution for RGBD sensors. Possible combinations with deep learning techniques will eventually be briefly discussed. 
  </p>

<p>
  6 december 2018, 14h-15h, <a href="https://hugues-talbot.github.io/">Hughes Talbot</a> (CentraleSupelec)<br />
  <b>Title:</b> <i>Path operators for thin objects restoration</i>
  
  <br />
  <b>Abstract:</b> In this talk we will present path operators, which are efficient recursive mathematical morphology connected operators that use paths as structuring elements. These operators are designed to preserve thin objects in images, such as hair, cilia, vessels, oriented textures, etc, which are traditionally very difficult to filter using classical operators in many settings. By combining these filters, we show how we can propose a vesselness operator with significant better performance than the traditional linear operators based on the Hessian (Frangi, Sato, etc) or the structure tensor. We also show recent work on how to use these operators as regularizers in variational frameworks for image restoration, in the context of discrete calculus. 
  </p>

<p>
  6 december 2018, 15h-16h, <a href="http://bigwww.epfl.ch/fortun/index.html">Denis Fortun</a> (iCUBE, CNRS, Universit√© de Strasbourg)<br />
  <b>Title:</b> <i>Fast piecewise-affine motion estimation without segmentation</i>
  
  <br />
  <b>Abstract:</b> In this talk, we will review existing strategies for regularizing motion fields, and present a new method dedicated to piecewise affine models. Current algorithmic approaches for piecewise affine motion estimation are based on alternating motion segmentation and estimation. In contrast, our method estimates piecewise affine motion directly without intermediate segmentation. To this end, we reformulate the problem by imposing piecewise constancy of the parameter field, and derive a specific proximal splitting optimization scheme. A key component of our framework is an efficient 1D piecewise-affine estimator for vector-valued signals. The first advantage of our approach over segmentation-based methods is its absence of initialization. The second advantage is its lower computational cost, which is independent of the complexity of the motion field. In addition to these features, we demonstrate competitive accuracy with other piecewise-parametric methods on standard evaluation benchmarks. Our new regularization scheme also outperforms the more standard use of total variation and total generalized variation. 
  </p>

<p>
  8 november 2018, 14h-15h, <a href="http://people.irisa.fr/Remi.Gribonval/">R√©mi Gribonval</a> (INRIA, Panama project-team)<br />
  <b>Title:</b> <i>Approximation with sparsely connected deep networks</i>
  
  <br />
  <b>Abstract:</b> Many of the data analysis and processing pipelines that have been carefully engineered by generations of mathematicians and practitioners can in fact be implemented as deep networks. Allowing the parameters of these networks to be automatically trained (or even randomized) allows to revisit certain classical constructions.
The talk first describes an empirical approach to approximate a given matrix by a fast linear transform through numerical optimization. The main idea is to write fast linear transforms as products of few sparse factors, and to iteratively optimize over the factors. This corresponds to training a sparsely connected, linear, deep neural network. Learning algorithms exploiting iterative hard-thresholding  have been shown to perform well in practice, a striking example being their ability to somehow ‚Äúreverse engineer‚Äù the fast Hadamard transform. Yet, developing a solid understanding of their conditions of success remains an open challenge.
In a second part, we study the expressivity of sparsely connected deep networks. Measuring a network's complexity by its number of connections, we consider the class of functions which error of best approximation with networks of a given complexity decays at a certain rate. Using classical approximation theory, we show that this class can be endowed with a norm that makes it a nice function space, called approximation space. We establish that the presence of certain ‚Äúskip connections‚Äù has no impact of the approximation space, and discuss the role of the network's nonlinearity (also known as activation function) on the resulting spaces, as well as the benefits of depth. For the popular ReLU nonlinearity (as well as its powers), we relate the newly identified spaces to classical Besov spaces, which have a long history as image models associated to sparse wavelet decompositions. The sharp embeddings that we establish highlight how depth enables sparsely connected networks to approximate functions of increased ‚Äúroughness‚Äù (decreased Besov smoothness) compared to shallow networks and wavelets.
Joint work with Luc Le Magoarou (Inria), Gitta Kutyniok (TU Berlin), Morten Nielsen (Aalborg University) and Felix Voigtlaender (KU Eichst√§tt).  
  </p>

<p>
  8 november 2018, 15h-16h, <a href="https://houdard.wp.imt.fr">Antoine Houdard</a> (Telecom ParisTech &amp; Universite Paris Descartes)<br />
  <b>Title:</b> <i>Some advances in patch-based image denoising</i>
  
  <br />
  <b>Abstract:</b> In this talk I will present my PhD thesis work on non-local methods for image denoising. Natural images contain redundant structures, and this property can be used for restoration purposes. A common way to consider this self-similarity is to separate the image into patches. These patches can then be grouped, compared and filtered together. The main part of this talk will be dedicated to the study of Gaussian priors for patch-based image denoising. Such priors are widely used for image restoration. We propose some ideas to answer the following questions: Why are Gaussian priors so widely used? What information do they encode about the image? Next I shall propose a probabilistic high-dimensional mixture model on the noisy patches. This model adopts a sparse modeling which assumes that the data lie on group-specific subspaces of low dimensionalities. This yields a denoising algorithm that demonstrates state-of-the-art performance. 
  </p>

<p>
  4 october 2018, 14h-15h, <a href="https://www.linkedin.com/in/charles-hessel-2aa70284/?originalSubdomain=fr">Charles Hessel</a> (DxO and CMLA, ENS Paris Saclay)<br />
  <b>Title:</b> <i>Base and detail decomposition filters and the measure of their artifacts</i>
  
  <br />
  <b>Abstract:</b> In this CIFRE thesis, a collaboration between the CMLA, ENS Paris-Saclay and the company DxO, we tackle the problem of the additive decomposition of an image into base and detail. Such a decomposition is a fundamental tool in image processing. For applications to professional photo editing in DxO Photolab, a core requirement is the absence of artifacts. For instance, in the context of contrast enhancement, in which the base is reduced and the detail increased, minor artifacts becomes highly visible. The distortions thus introduced are unacceptable from the point of view of a photographer. The objective of this thesis is to single out and study the most suitable filters to perform this task, to improve the best ones and to define new ones. This requires a rigorous measure of the quality of the base plus detail decomposition. We examine two classic artifacts (halo and staircasing) and discover three more sorts that are equally crucial: contrast halo, compartmentalization, and the dark halo. This leads us to construct five adapted patterns to measure these artifacts. We end up ranking the optimal filters based on these measurements, and arrive at a clear decision about the best filters. Two filters stand out, including one we propose. 
  </p>

<p>
  4 october 2018, 15h-16h, <a href="https://www.math.ens.fr/fiche-membre/fiche-membre.php?id_membre=361">Paul Catala</a> (ENS)<br />
  <b>Title:</b> <i>A Low-Rank Approach to Off-The-Grid Sparse Deconvolution</i>
  
  <br />
  <b>Abstract:</b> In this talk, I will present a new solver for the sparse spikes deconvolution problem over the space of Radon measures. A common approach to off-the-grid deconvolution considers semidefinite (SDP) relaxations of the total variation (the total mass of the absolute value of the measure) minimization problem. The direct resolution of this SDP is however intractable for large scale settings, since the problem size grows as n^2d where n is the cutoff frequency of the filter and d the ambient dimension. I will first introduce a penalized formulation of this semidefinite lifting, which has low-rank solutions. This formulation is then solved using a conditional gradient optimization scheme with non-convex updates. This algorithm leverages both the low-rank and the convolutive structure of the problem, resulting in an O(n^d log n) complexity per iteration. Numerical simulations are promising and show that the algorithm converges in exactly r steps, r being the number of Diracs composing the solution. 
  </p>

<p>
  14 juin 2018, 14h-15h, <a href="https://iie.fing.edu.uy/~pmuse/">Pablo Mus√©</a> (Facultad de Ingenier√≠a, Universidad de la Rep√∫blica, Montevideo, Uruguay)<br />
  <b>Title:</b> <i>OL√â, Orthogonal Low-rank Embedding, A Novel Approach for Deep Metric Learning</i>
  
  <br />
  <b>Abstract:</b> Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are common tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, such solutions carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this talk we present a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss. Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative and more robust. This is a joint work with Jos√© Lezama, Qiang Qiu and Guillermo Sapiro 
  </p>

<p>
  14 juin 2018, 15h-16h, <a href="https://www.math.univ-toulouse.fr/~pescande/index.html">Paul Escande</a> (Johns Hopkins University)<br />
  <b>Title:</b> <i>Multi-scale Decomposition of Transformations (MUSCADET)</i>
  
  <br />
  <b>Abstract:</b> In many applications, transformations between two domains are defined through point-wise mappings. These functions can be costly to store and compute, but also hard to interpret in a geometric fashion. In this work, we propose a way to overcome these difficulties. The main idea is a novel multi-scale decomposition of complex transformations into a cascade of elementary, user-specified, transformations. This methods allows to (i) Construct efficient approximations for elements of large spaces of complex transformations using simple understandable blocks, (ii) Use transformations to measure similarities between complex objects, (iii) Deal with invariance under certain transformations, (iv) Perform statistical inference tasks on sets of transformations. We will describe the method as well as provide theoretical guarantees on the quality of the multi-scale approximations. Then we will present some numerical experiments that show its computational efficiency. 
  </p>

<p>
  03 mai 2018, 14h-15h, <a href="https://www.lri.fr/~gcharpia/">Guillaume Charpiat</a> (√âquipe TAO - INRIA Saclay)<br />
  <b>Title:</b> <i>Introduction to Neural Networks</i>
  
  <br />
  <b>Abstract:</b> Neural networks have become extremely popular these last years, notably due to their recent impressive successes in computer vision, under the name of deep learning. This tutorial will describe the main principles and properties of neural networks, with a focus on convolutional neural networks (CNN), particularly suited for image-based machine learning tasks. Depending on the audience, and if time permits, we may also cover topics such as auto-encoders, generative adversarial networks, or style transfer. 
  </p>

<p>
  03 mai 2018, 15h-16h, <a href="https://imsc.uni-graz.at/hollerm/">Martin Holler</a> (CMAP, Ecole Polytechnique)<br />
  <b>Title:</b> <i>Analysis and applications of coupled regularization with multiple data discrepancies</i>
  
  <br />
  <b>Abstract:</b> In many applications of inverse problem in imaging, the measured data does not correspond to a single measurement but rather to multiple simultaneous or sequential measurements, featuring different forward models and/or noise characteristics. Examples of such a setting are the joint acquisition of magnetic resonance (MR) and positron emission tomography (PET) images or the sequential acquisition of multiple time frames in dynamic imaging. Assuming that the images one aims to reconstruct from such measurements have different but related content, coupled regularization techniques aim at exploiting such correlations for improved reconstruction. A corresponding variational formulation comprises multiple potentially different data discrepancies and raises the question of how standard stability and convergence results in inverse problems transfer to such a situation. In this talk, we address this question. Motivated by concrete applications with different noise characteristics, we first consider a rather general setting and in particular show how the adaption of parameter choices strategies to different discrepancy terms yields improved convergence results. We then further elaborate on practically relevant special cases and show numerical results for joint MR-PET reconstruction and multi-spectral electron microscopy. 
  </p>

<p>
  05 avril 2018, 14h-15h, <a href="https://nkeriven.github.io/">Nicolas Keriven</a> (ENS)<br />
  <b>Title:</b> <i>Sketched Learning from Random Features Moments</i>
  
    <b>[<a href="../slides/keriven.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Learning parameters from voluminous data can be prohibitive in terms of memory and computational requirements. Furthermore, modern architectures often ask for learning methods to be amenable to streaming or distributed computing. In this context, a popular approach is to first compress the database into a representation called a linear sketch, then learn the desired information using only this sketch. In this talk, we introduce a methodology to fit a mixture of probability distributions on the data, using only a sketch of the database. The sketch is defined by combining two notions from the reproducing kernel literature, kernel mean embedding and random features. It is seen to correspond to linear measurements of the probability distribution of the data, and the problem is thus analyzed under the lens of Compressive Sensing (CS), in which a signal is randomly measured and recovered. We analyze the problem using two classical approaches in CS: first a Restricted Isometry Property in the Banach space of finite signed measures, from which we obtain strong recovery guarantees however with an intractable non-convex minimization problem, and second with a dual certificate analysis, from which we show that total-variation regularization yields a convex minimization problem that in some cases recovers exactly the number of components of a gaussian mixture model. We also briefly describe a flexible heuristic greedy algorithm to estimate mixture models from a sketch, and apply it on synthetic and real data. 
  </p>

<p>
  05 avril 2018, 15h-16h, <a href="http://lucacalatroni.weebly.com/">Luca Calatroni</a> (CMAP, Ecole Polytechnique)<br />
  <b>Title:</b> <i>A variational model for mixed noise removal: analysis, optimisation and structure of solutions</i>
  
  <br />
  <b>Abstract:</b> In several real-word imaging applications such as microscopy, astronomy and medical imaging, a combination of transmission and acquisition faults result in multiple noise statistics in the observed image, such as impulsive/Gaussian or Gaussian/Possion mixtures.  By means of a joint MAP estimation, we derive a statistically consistent variational model where single data fidelities are combined in a handy infimal convoution fashion to model the noise mixture and separated from each other via a Total Variation smoothing. By means of a fine analysis in suitable function spaces, we then study the structure of the solutions of the corresponding variational model and propose a bilevel optimisation strategy for the estimation of the optimal regularisation weights. This is joint work with C.B. Sch√∂nlieb (University of Cambridge, UK), J.C. De Los Reyes (ModeMat, Quito, Ecuador)  and K. Papafitsoros (WIAS Institue, Berlin, Germany). 
  </p>

<p>
  08 mars 2018, 14h-15h, <a href="http://perso.ens-lyon.fr/nelly.pustelnik/">Nelly Pustelnik</a> (CNRS, Laboratoire de Physique - CNRS UMR 5672 -- ENS Lyon)<br />
  <b>Title:</b> <i>Analyse multir√©solution et optimisation non lisse pour la segmentation de texture</i>
  
  <br />
  <b>Abstract:</b> La segmentation d'images textur√©es continue de pr√©senter un challenge majeur en traitement d'images quand les textures rencontr√©es sont de type stochastiques. Dans cet expos√©, nous aborderons cette question par le couplage entre analyse multir√©solution et outils d‚Äôoptimisation non lisse. Nous pr√©senterons d'une part les approches usuelles en deux temps estimation/segmentation puis nous expliciterons les mod√®les d√©velopp√©s permettant d'effectuer les deux √©tapes de fa√ßon jointe. D'autre part, nous nous int√©resserons √† affiner ces proc√©dures de segmentation de textures d'un point de vue algorithmique pour obtenir des m√©thodes √† faible co√ªt calculatoire de fa√ßon √† √©valuer les performances des outils d√©velopp√©s sur les grands volumes de donn√©es tels que ceux rencontr√©s dans l‚Äô√©tude de la dynamique des √©coulements multiphasiques dans des milieux poreux. 
  </p>

<p>
  08 mars 2018, 15h-16h, <a href="http://gris.perso.math.cnrs.fr/fr/">Barbara Gris</a> (KTH Royal Institute of Technology in Stockholm)<br />
  <b>Title:</b> <i>Reconstruction d'image √† l'aide d'un a priori de d√©formation</i>
  
  <br />
  <b>Abstract:</b> La tomographie est une technique d'imagerie m√©dicale qui consiste √† reconstruire le volume d‚Äôun objet √† partir de ses projections. Lorsque l'acquisition des donn√©es est longue, le sujet peut effectuer des mouvements (par exemple respiratoires) qui provoquent des artefacts dans l'image reconstruite. Le mod√®le que je propose a pour but de tenir compte des mouvements possibles afin d'aider √† la reconstruction de l'image. Une premi√®re √©tape est de reconstruire  cette image comme d√©formation d'une image template suppos√©e connue, tout en incorporant un a priori dans les d√©formations possibles. Je pr√©senterai la notion de module de d√©formation et montrerai comment elle permet de contraindre les d√©formations √† respecter une certaine structure (donn√©e par exemple des contraintes physiques) tout en laissant certains param√®tres libres afin qu'elles puissent s'adapter aux donn√©es. 
  </p>

<p>
  08 f√©vrier 2018, 14h-15h, <a href="https://lchizat.github.io">L√©na√Øc Chizat</a> (L√©na√Øc Chizat (SIERRA team, INRIA))<br />
  <b>Title:</b> <i>A tutorial on optimal transport, part I: theory, model, properties</i>
  
    <b>[<a href="../slides/chizat.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> The optimal transport (OT) problem is often described as that of finding the most efficient way of moving a pile of dirt from one configuration to another. Once stated formally, OT provides extremely useful tools for comparing, interpolating and processing objects such as distributions of mass, probability measures, histograms or densities. This talk is an up-to-date tutorial on a selection of topics in OT. In the first part, we will present an intuitive description on OT, its behaviour and main properties. In the second part, we will introduce state-of-the-art numerical methods for solving OT (based on entropic regularization) and present how this tool can be used for both imaging and machine learning problems. 
  </p>

<p>
  08 f√©vrier 2018, 15h-16h, <a href="https://audeg.github.io">Aude Genevay</a> (Aude Genevay (Ecole Normale Sup√©rieure et Universit√© Paris Dauphine))<br />
  <b>Title:</b> <i>A tutorial on optimal transport, part II: Optimal transport for machine learning</i>
  
    <b>[<a href="../slides/genevay.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> The optimal transport (OT) problem is often described as that of finding the most efficient way of moving a pile of dirt from one configuration to another. Once stated formally, OT provides extremely useful tools for comparing, interpolating and processing objects such as distributions of mass, probability measures, histograms or densities. This talk is an up-to-date tutorial on a selection of topics in OT. In the first part, we will present an intuitive description on OT, its behaviour and main properties. In the second part, we will introduce state-of-the-art numerical methods for solving OT (based on entropic regularization) and present how this tool can be used for both imaging and machine learning problems. 
  </p>

<p>
  11 janvier 2018, 14h-15h, <a href="https://edouardoyallon.github.io/">Edouard Oyallon</a> (CentraleSupelec)<br />
  <b>Title:</b> <i>Deep CNNs: the end of prior features?</i>
  
  <br />
  <b>Abstract:</b> Since 2012, Deep Convolutional Neural Networks provide generic and robust methods, that replaced predefined representations like SIFTs or HoGs in many state-of-the-art applications. In this talk, we show that supervised CNNs can be improved by incorporating geometric priors like a Scattering Transform: for instance, they learn with less samples, and are more interpretable. 
  </p>

<p>
  11 janvier 2018, 15h-16h, <a href="http://www.math-info.univ-paris5.fr/~aleclair/">Arthur Leclaire</a> (CMLA, ENS Paris-Saclay)<br />
  <b>Title:</b> <i>Semi-discrete optimal transport in patch space for structured texture synthesis</i>
  
  <br />
  <b>Abstract:</b> In this talk we address exemplar-based texture synthesis using a model obtained as a local transform of a Gaussian random field. The local transformation is designed to solve a semi-discrete optimal transport problem in the patch space in order to reimpose the patch distribution of the exemplar texture. Since the patch space is high-dimensional, the optimal transport problem is solved with a stochastic optimization procedure. The resulting model inherits several benefits of the Gaussian model (stationarity, mid-range correlations) with an additional statistical guarantee on the patch distribution. We will also propose a multiscale extension of this model, which allows to synthesize structured textures with low requirements in terms of time and memory storage. 
  </p>

<p>
  7 d√©cembre 2017, 14h-15h, <a href="https://sites.google.com/site/irenekaltenmark/">Irene Kaltenmark</a> (Institut de Neurosciences de La Timone - Universit√© d'Aix-Marseille)<br />
  <b>Title:</b> <i>Mod√®les g√©om√©triques de croissance en anatomie computationnelle</i>
  
  <br />
  <b>Abstract:</b> L‚Äôutilisation de groupes de diff√©omorphismes agissant sur des ensembles de formes, √©quipant ces derniers d‚Äôune structure riemannienne, s‚Äôest av√©r√©e extr√™mement efficace pour mod√©liser et analyser la variabilit√© de populations de formes issues de donn√©es d‚Äôimagerie m√©dicale. N√©anmoins, √† l'int√©gration de l'analyse longitudinale des donn√©es, ont √©merg√© des ph√©nom√®nes biologiques de croissance ou de d√©g√©n√©rescence se manifestant par des d√©formations de nature non diff√©omorphique. La croissance d'un organisme par adjonction progressive et localis√©e de nouvelles mol√©cules, √† l‚Äôinstar d‚Äôun processus de cristallisation, ne s'apparente pas √† un simple √©tirement du tissu initial. Face √† cette observation, nous proposons de garder l'esprit g√©om√©trique qui fait la puissance des approches diff√©omorphiques dans les espaces de formes mais en introduisant un concept assez g√©n√©ral de d√©ploiement o√π l'on mod√©lise les ph√©nom√®nes de croissance comme le d√©ploiement optimal progressif d‚Äôune forme pr√©alablement repli√©e dans une r√©gion de l'espace. √Ä la question d√©licate de la caract√©risation des appariements partiels mod√©lisant le d√©ploiement de la forme, nous r√©pondons par un syst√®me de coordonn√©es biologiques √©volutif et nous aboutissons finalement √† un nouveau probl√®me de contr√¥le optimal pour l'assimilation de donn√©es de surfaces √©voluant dans le temps et repr√©sent√©es par des courants ou des varifolds. 
  </p>

<p>
  7 d√©cembre 2017, 15h-16h, <a href="http://cazencott.info/">Chlo√©-Agathe Azencott</a> (CBIO -- Institut Mines-ParisTech, Institut Curie \&amp; INSERM)<br />
  <b>Title:</b> <i>Structured feature selection in high dimension for precision medicine</i>
  
  <br />
  <b>Abstract:</b> Differences in disease predisposition or response to treatment can be explained in great part by genomic differences between individuals. This realization has given birth to precision medicine, where treatment is tailored to the genome of patients. This field depends on collecting considerable amounts of molecular data for large numbers of individuals, which is being enabled by thriving developments in genome sequencing and other high-throughput experimental technologies. Unfortunately, we still lack effective methods to reliably detect, from this data, which of the genomic features determine a phenotype such as disease predisposition or response to treatment. One of the major issues is that the number of features that can be measured is large (easily reaching tens of millions) with respect to the number of samples for which they can be collected (more usually of the order of hundreds or thousands), posing both computational and statistical difficulties. In my talk I will discuss several ways to use constraints on the feature selection procedure to address this problem. 
  </p>

<p>
  9 Novembre 2017, 14h-15h, <a href="https://sites.google.com/view/paulinetan">Pauline Tan</a> (ONERA \&amp; CMLA - ENS Paris Saclay)<br />
  <b>Title:</b> <i>Alternating proximal gradient descent for nonconvex regularised problems with biconvex and multiconvex coupling terms</i>
  
  <br />
  <b>Abstract:</b>  There has been an increasing interest in constrained nonconvex  regularized block biconvex / multiconvex optimization problems. We introduce an  approach that effectively exploits the biconvex / multiconvex structure of the coupling term and enables complex application-dependent regularization terms to be used.The proposed ASAP algorithm enjoys simple well defined updates. Global convergence of the algorithm to a critical point is proved using the so-called Kurdyka-Lojasiewicz  property for subanalytic functions. Moreover, we prove that a large class of useful objective functions obeying our assumptions are subanalytic and thus satisfy the Kurdyka-Lojasiewicz property. I will also present two particular applications of the algorithm to big-data air-born sequences of images, which are already used by our industrial partner ONERA.
This is a joint work with Mila Nikolova (CMLA, CNRS, ENS Paris-Saclay).  
  </p>

<p>
  9 Novembre 2017, 15h-16h, <a href="http://chercheurs.lille.inria.fr/ekaufman">Emilie Kaufmann</a> (CNRS, INRIA Lille, Universit√© de Lille)<br />
  <b>Title:</b> <i>A tutorial on Multi-Armed Bandit problems, Theory and Practice</i>
  
  <br />
  <b>Abstract:</b> A Multi-Armed Bandit (MAB) model is a simple framework in which an agent sequentially sample arms, that are unknown probability distributions, in order to learn something about these underlying distributions, possibly under the constraint of maximizing some notion of reward. Stochastic MABs have been introduced in the 1930s as a simple model for clinical trials, and are widely studied nowadays for several applications, that range from sequential content optimization, cognitive radios or the design of AI for games. In this introduction to MAB, we will review existing (efficient) algorithms to either achieve an exploration/exploitation trade-off or to optimally explore a simple, i.i.d., stochastic environment. We will then see how these algorithms can be extended to deal with more realistic applications. 
  </p>

<p>
  5 Octobre 2017, 14h-15h, <a href="https://www.math.univ-toulouse.fr/~fmalgouy/">Fran√ßois Malgouyres</a> (MIP - Universit√© Paul Sabatier, Toulouse)<br />
  <b>Title:</b> <i>Stable recovery of the factors from a deep matrix product and application to convolutional network</i>
  
  <br />
  <b>Abstract:</b> We study a deep matrix factorization problem. It takes as input a matrix \(X\) obtained by multiplying \(K\) matrices (called factors). Each factor is obtained by applying a fixed linear operator to a short vector of parameters satisfying a model (for instance sparsity, grouped sparsity, non-negativity, constraints defining a convolution network...  ). We call the problem deep or multi-layer because the number of factors is not limited. In the practical situations we have in mind, we can typically have \(K=10\) or \(100\). This work aims at identifying  conditions on the structure of the model that guarantees the stable recovery of the factors from the knowledge of \(X\) and the model for the factors.  We provide necessary and sufficient conditions for the identifiability of the factors (up to a scale rearrangement). We also provide a necessary and sufficient condition called Deep Null-Space-Property (because of the analogy with the usual Null Space Property in the compressed sensing framework) which guarantees that even an inaccurate optimization algorithm for  the factorization stably recovers the factors. We illustrate the theory with a practical example where the deep factorization is a linear convolutional network. 
  </p>

<p>
  5 Octobre 2017, 15h-16h, <a href="http://www.fabienpierre.fr/">Fabien Pierre</a> (LORIA - Universit√© de Lorraine)<br />
  <b>Title:</b> <i>Colorisation de vid√©os, de l'√©tat-de-l'art aux d√©bouch√©s industriels</i>
  
  <br />
  <b>Abstract:</b> La colorisation d'image est un probl√®me extr√™mement mal pos√© mais qui int√©resse l'industrie du divertissement. Ce double point de vue en fait un sujet tr√®s attractif. Dans cet expos√©, on pr√©sentera l'√©tat de l'art et les m√©thodes qui ont √©t√© d√©velopp√©es par l'orateur pendant sa th√®se. Celles-ci reposent sur des approches non-locales et variationnelles. Les fonctionnelles utilis√©es sont non-lisses et non-convexes et ont fait l'objet de techniques de minimisation originales. Cela a permis d'impl√©menter un logiciel exp√©rimental qui associe l'utilisateur √† une approche bas√©e-exemple ce qui donne une m√©thode efficace, flexible et rapide. Une extension √† la vid√©o est propos√©e, dont l'impl√©mentation en GPU permet une interactivit√© de l'approche variationnelle avec l'utilisateur. N√©anmoins, celle-ci n'est pas op√©rationnelle aux yeux d'experts du milieu de la colorisation. En vue de se conformer √† ces besoins, quelques pistes seront propos√©es. 
  </p>

<p>
  1er Juin 2017, 14h-15h, <a href="http://w3.mi.parisdescartes.fr/~cbouveyr/">Charles Bouveyron</a> (MAP5 - Universit√© Paris Descartes)<br />
  <b>Title:</b> <i>High-Dimensional Mixture Models for Unsupervised Image Denoising</i>
  
  <br />
  <b>Abstract:</b> This work addresses the problem of patch-based single image denoising through the unsupervised learning of a probabilistic high-dimensional mixture models on the noisy patches. The model, named hereafter HDMI, proposes a full modeling of the process that is supposed to have generated the noisy patches. To overcome the potential estimation problems due to the high dimension of the patches, the HDMI model adopts a parsimonious modeling which assumes that the data live in group-specific subspaces of low dimensionalities. This parsimonious modeling allows in turn to get a numerically stable computation of the conditional expectation of the image which is applied for denoising. The use of such a model also permits to rely on model selection tools, such as BIC, to automatically determine the intrinsic dimensions of the subspaces and the variance of the noise. This yields a blind denoising algorithm that demonstrates state-of-the-art performance, both when the noise level is known and unknown. Joint work with A. Houdard (T√©l√©com ParisTech) and J. Delon (MAP5 - Paris Descartes). 
  </p>

<p>
  1er Juin 2017, 15h-16h, <a href="http://www-pequan.lip6.fr/~tierny/">Julien Tierny</a> (CNRS et LIP6)<br />
  <b>Title:</b> <i>Topological Data Analysis for Scientific Visualization.</i>
  
  <br />
  <b>Abstract:</b> Scientific visualization aims at helping users (i) represent, (ii) explore, and (iii) analyze acquired or simulated geometrical data, for interpretation, validation or communication purposes. Among the existing techniques, algorithms inspired by Morse theory have demonstrated their utility in this context for the efficient and robust extraction of geometrical features, at multiple scales of importance. In this talk, I will give a brief tutorial on the topological methods used in scientific visualization for the analysis of scalar data. I will present algorithms with practical efficiency for the computation of topological abstractions (Reeb graphs, Morse-Smale complexes, persistence diagrams, etc.) in low dimensions (typically 2 or 3). I will also illustrate these notions with concrete use cases in astrophysics, fluid dynamics, molecular chemistry or combustion. I will also present the "Topology ToolKit" (topology-tool-kit.github.io), a recently released open-source library for topological data analysis, which implements most of the algorithms described above. I will give a brief usage tutorial, both for end-users and developers. I will also describe how easily it can be extended to disseminate research code. Finally,I will discuss perspectives, both from a research and implementation point of view. 
  </p>

<p>
  04 Mai 2017, 14h-15h, <a href="http://perso-math.univ-mlv.fr/users/jaffard.stephane/">St√©phane Jaffard</a> (Paris Est)<br />
  <b>Title:</b> <i>Analyse multifractale pour la classification d'images.</i>
  
  <br />
  <b>Abstract:</b> L'analyse multifractale a √©t√© introduite √† la fin des ann√©es 1980 par des physiciens dont le but √©tait de relier les indices de r√©gularit√© globale d'un signal (la vitesse d'un fluide turbulent), avec la distribution des singularit√©s ponctuelles pr√©sentes dans les donn√©es. Diff√©rentes variantes de la m√©thode existent, bas√©es sur les sup locaux d'une transform√©e continue en ondelettes, ou sur la DFA (Detrented Fluctuation Analysis). Nous consid√©rerons d'autres versions, construites √† partir des coefficients sur une base orthonorm√©e d'ondelettes.  Nous verrons comment les outils fournis par l'analyse multifractale  peuvent √™tre adapt√©s √† diff√©rents types de donn√©es; utilisation des ``p-leaders'' (normes \(\ell^p\) locales de coefficients d'odelettes) √† la place des 'leaders' (sup locaux de coefficients d'ondelettes) pour des donn√©es peu r√©guli√®res, ou encore ondelettes anisotropes pour l'analyse de textures anisotropes. Nous verrons aussi comment adapter l'analyse quand  les donn√©es ne pr√©sentent pas d'autosimilarit√©.  Les exemples illustrant ces m√©thode seront tir√©s (en 1D) de  la turbulence, le trafic internet, le rythme cardiaque, les textes litt√©raires, et (en 2D), des images naturelles, des peintures et des papiers photographiques anciens. En ce qui concerne les textes litt√©raires et les peintures, nous verrons en quoi ces m√©thodes permettent de fournir de nouveaux outils en textom√©trie et en stylom√©trie. 
  </p>

<p>
  04 Mai 2017, 15h-16h, <a href="https://balle.io/">Johannes Ball√©</a> (Google &amp; New York University)<br />
  <b>Title:</b> <i>The importance of local gain control.</i>
  
    <b>[<a href="../slides/balle.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Local gain control is ubiquitous in biological sensory systems and leads, for example, to masking effects in the visual system. When modeled as an operation known as divisive normalization, it represents an invertible nonlinear transformation, and has several interesting properties useful for image processing. We introduce a generalized version of the transform (GDN), and use it to construct a novel visual quality metric which outperforms MS-SSIM in predicting human distortion assessments. We also show it can be used to Gaussianize image densities, yielding factorized representations, and providing probabilistic image models superior to sparse representations. Finally, we use it to design a simple image compression method, yielding compression quality which is visually close to the state of the art. 
  </p>

<p>
  30 Mars 2017, 14h-15h, <a href="http://www.lsta.upmc.fr/boyer/">Claire Boyer</a> (UPMC)<br />
  <b>Title:</b> <i>Adapting to unknown noise level in super-resolution.</i>
  
  <br />
  <b>Abstract:</b> We study sparse spikes deconvolution over the space of complex-valued measures when the input measure is a finite sum of Dirac masses. We introduce a new procedure to handle the spike deconvolution when the noise level is unknown. Prediction and localization results will be presented for this approach. An insight on the probabilistic tools used in the proofs could be briefly given as well. 
  </p>

<p>
  30 Mars 2017, 15h-16h, <a href="https://www.math.u-bordeaux.fr/~npapadak/">Nicolas Papadakis</a> (CNRS et Bordeaux 1)<br />
  <b>Title:</b> <i>Covariant LEAst-Square Re-fitting for image restoration.</i>
  
  <br />
  <b>Abstract:</b> We propose a new framework to remove parts of the systematic errors affecting popular restoration algorithms, with a special focus for image processing tasks. Generalizing ideas that emerged for l1 regularization, we develop an approach re-fitting the results of standard methods towards the input data. Total variation regularizations and non-local means are special cases of interest. We identify important covariant information that should be preserved by the re-fitting method, and emphasize the importance of preserving the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we provide an approach that has a ``twicing'' flavor and allows re-fitting the restored signal by adding back a local affine transformation of the residual term. We illustrate the benefits of our method on numerical simulations for image restoration tasks. Joint work with C.-A. Deledalle (IMBordeaux), J. Salmon (TELECOM ParisTech) and S. Vaiter (IMBourgogne). 
  </p>

<p>
  2 Mars 2017, 14h-15h, <a href="http://www.technicolor.com/en/patrick-perez">Patrick Perez</a> (Technicolor)<br />
  <b>Title:</b> <i>Signaux sur graphe, du traitement √† l'apprentissage.</i>
  
    <b>[<a href="../slides/perez.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Motiv√©es par la profusion de signaux int√©ressants qui sont attach√©s un graphe (un r√©seau de transport, un r√©seau social, un maillage 3D) ou dont la structure interne est bien capt√©e par un graphe entre ses parties (un image, un son), des √©tudes visant √† √©tendre aux graphes les outils classiques de la th√©orie et du traitement des signaux ont vu le jour dans un pass√© r√©cent. Nous rappellerons les bases de telles extensions, en particulier au moyen de l'analyse spectrale de graphe, pour nous concentrer ensuite sur plusieurs probl√®mes et applications; (1) L'√©chantillonnage al√©atoire de signaux sur graphe et la reconstruction √† partir des √©chantillons obtenus avec application aux super-pixels d'une image; (2)  L'extraction et la r√©gression de corrections harmoniques de maillages param√©triques avec application √† la mod√©lisation de visages; (3) L'unification de traitements locaux et non-locaux de signaux sur graphe au moyen de r√©seaux convolutifs al√©atoires ou appris, avec application au d√©bruitage et √† l'√©dition d'images. 
  </p>

<p>
  2 Mars 2017, 15h-16h, <a href="http://www-ljk.imag.fr/membres/Valerie.Perrier/">Val√©rie Perrier</a> (LJK)<br />
  <b>Title:</b> <i>Application des ondelettes √† divergence nulle pour le transport optimal</i>
  
  <br />
  <b>Abstract:</b> Dans de nombreuses applications, la solution du probl√®me est  un champ de vecteur qui doit v√©rifier une condition de divergence nulle : c'est le cas des champs de vitesse incompressibles solutions des √©quations de Navier-Stokes, ou du champ magn√©tique pour les solutions de Maxwell. Plus r√©cemment, les champs √† divergence nulle ont trouv√© d'autres applications, comme la compression de champs de vecteur en infographie, ou encore la r√©solution du transport optimal dans sa formulation dynamique. Dans cet expos√©, nous int√©ressons √† la d√©composition des champs √† divergence nulle v√©rifiant des conditions aux limites "physiques" : pour cela nous introduisons une nouvelle base d'ondelettes √† divergence nulle sur le carr√© ou le cube, qui diagonalise les op√©rateurs de d√©rivation. En particulier sur cette base, la complexit√© pour r√©soudre un Laplacien-Dirichlet avec condition de divergence nulle est optimale (lin√©aire). Dans un deuxi√®me temps, nous consid√©rons la formulation du transport optimal dynamique de Benamou-Brenier, que nous reformulons sur un espace de contraintes √† divergence nulle. La minimisation de la fonctionnelle est alors effectu√©e par une descente de gradient sur l'espace des coefficients d'ondelettes √† divergence nulle, et uniquement gr√¢ce √† des d√©compositions-recompositions sur ondelettes. Ce travail est effectu√© en collaboration avec Morgane Henri, Souleymane Kadri-Harouna (universit√© de La Rochelle) et Emmanuel Ma√Ætre. 
  </p>

<p>
  2 F√©vrier 2017, 14h-15h, <a href="https://www.i2m.univ-amu.fr/~caroline.chaux/">Caroline Chaux</a> (CNRS et I2M)<br />
  <b>Title:</b> <i>Nonnegative Tensor Factorization using a proximal algorithm, application to 3D fluorescence spectroscopy.</i>
  
    <b>[<a href="../slides/chaux.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> This is a Joint work with Xuan Vu, Nad√®ge Thirion-Moreau and Sylvain Maire (LSIS, Toulon). We address the problem of third order nonnegative tensor factorization with penalization. More precisely, the Canonical Polyadic Decomposition (CPD) is considered. It constitutes a compact and informative model consisting of decomposing a tensor into a minimal sum of rank-one terms.  This multi-linear decomposition has been widely studied in the litterature. Coupled with 3D fluorescence spectroscopy analysis, it has found numerous interesting applications in chemistry, chemometrics, data analysis for the environment, monitoring and so on. The resulting inverse problem at hand is often hard to solve especially when the tensor rank is unknown and when data corrupted by noise and large dimensions are considered. We adopted a variational approach and the factorization problem is thus formulated under a penalized minimization problem.  Indeed, a new penalized nonnegative third order CPD algorithm has been derived based on a block coordinate variable metric forward-backward method. The proposed iterative algorithm have been successfully applied not only to synthetic data (showing its efficiency, robustness and flexibility) but also on real 3D fluorescence spectroscopy data. 
  </p>

<p>
  2 F√©vrier 2017, 15h-16h, <a href="http://math.univ-lyon1.fr/homes-www/masnou/">Simon Masnou</a> (Institut Camille Jordan)<br />
  <b>Title:</b> <i>Reconstruction de volume √† partir de coupes.</i>
  
    <b>[<a href="../slides/masnou.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Le probl√®me de reconstruire un volume 3D √† partir de coupes 2D est fr√©quent dans de nombreuses applications en imagerie m√©dicale ou en infographie. La principale difficult√© est d'incorporer les contraintes car, en fonction du contexte, on peut parfois vouloir imposer des contraintes strictes, et d'autres fois conserver une certaine libert√© en cas de donn√©es bruit√©es ou impr√©cises. Je pr√©senterai des r√©sultats r√©cents que nous avons obtenus pour ce probl√®me avec Elie Bretin et Fran√ßois Dayrens. Notre approche repose sur un mod√®le variationnel utilisant un terme de r√©gularisation g√©om√©trique (tel que le p√©rim√®tre ou une √©nergie faisant intervenir la courbure) coupl√© √† des contraintes de densit√© pour les coupes. Nous avons d√©montr√© que ce mod√®le peut √™tre bien approch√© par des √©nergies r√©guli√®res √† l'aide d'une m√©thode de champ de phase et nous avons propos√© un sch√©ma num√©rique efficace et pr√©cis pour son approximation num√©rique. Je pr√©senterai les r√©sultats que nous avons obtenus pour des contraintes vari√©es, coupes planaires ou non planaires, parall√®les ou non parall√®les, surfaciques ou ponctuelles, etc. La m√©thode peut √™tre √©tendue √† des volumes multiples, ce qui est notamment int√©ressant pour la reconstruction de donn√©es segment√©es. 
  </p>

<p>
  5 Janvier 2017, 14h-15h, <a href="http://www.i2m.univ-amu.fr/~santhoine/">Sandrine Anthoine</a> (CNRS et I2M)<br />
  <b>Title:</b> <i>Generalized greedy algorithms</i>
  
    <b>[<a href="../slides/anthoine.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Matching Pursuit or CoSaMP are classical algorithms in signal processing that seek the best \(k\)-term approximation of a signal on a specified dictionary. Matching Pursuit is greedy in the sense that it chooses the atoms that enter the decomposition one at a time. Its descendants, such as CoSaMP or Subspace Pursuit, do not exactly choose one atom at a time but still aim at pinpointing the support of length \(k\) exactly of the solution. By opposition to convex relaxation alternatives, such as \(\ell_1\) penalized solutions, which do not seek an exactly \(k\)-sparse solution, we generally term Matching Pursuit, and its descendants "greedy". In approximation theory, the notion of "best" approximation is naturally in the sense of the \(\ell_2\) norm. Hence greedy algorithms are designed to find the \(k\)-sparse element that minimizes the \(\ell_2\) discrepancy. By contrast with convex relaxation, it is not easy to extend their scope to other discrepancies and obtain convergence guarantees. In this work, we propose to extend the scope of four greedy algorithms, Subspace Pursuit, CoSaMP, Orthogonal Matching Pursuit with Replacement and Iterative Hard Thresholding to the problem of findings zeros of operators in a Hilbert space. To do so we design the "Restricted Diagonal Property", which, as the "Restricted Isometry Property" in the classical case, ensures the good behavior of the algorithms. We are thus able for example to use these algorithms to find sparse critical points of functions that are neither convex nor concave. We finally give examples that illustrate the method. This is joint work with F.-X. Dup√© (LIF). 
  </p>

<p>
  5 Janvier 2017, 15h-16h, <a href="http://www.math.u-psud.fr/~mirebeau/">Jean-Marie Mirebeau</a> (CNRS, labo de math√©matiques d'Orsay)<br />
  <b>Title:</b> <i>Calcul de chemins minimaux avec p√©nalisation de courbure, via l'algorithme du Fast Marching. Applications en segmentation d'images.</i>
  
    <b>[<a href="../slides/mirebeau.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Nous consid√©rons des mod√®les de plus courts chemins avec p√©nalisation de courbure, tels que les √©lasticas d'Euler/Mumford, ou la voiture de Reed-Shepp avec ou sans marche arri√®re. Pour calculer le chemin d'√©n√©rgie minimale joignant deux points donn√©s, nous approchons ces mod√®les singuliers √† l'aide de m√©triques Riemanniennes ou Finsleriennes fortement anisotropes sur l'espace produit \(\mathbb{R}^d \times S^{d-1}\). Les √©quations eikonales associ√©es sont ensuites r√©solues via des variantes sp√©cialis√©es de l'algorithme du Fast-Marching. Nous pr√©sentons des applications √† la segmentation de structures tubulaires dans les images m√©dicales. 
  </p>

<p>
  24 Novembre 2016, 14h-15h, <a href="https://sites.google.com/site/jeanmichelmorelcmlaenscachan/">Jean-Michel Morel</a> (ENS Cachan)<br />
  <b>Title:</b> <i>The ego-motion scale space.</i>
  
    <b>[<a href="../slides/morel.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> This is a joint work with Javier S√°nchez P√©rez (Universidad de Las Palmas Gran Canaria). We address the homographic stabilization of video. This is the process by which the jitter of a moving camera is being compensated automatically from the video itself, in absence of external calibration information like the  one that would be provided by accelerometers or gyroscopes. I will discuss the various modes to define video stabilization. Then I will display several examples illustrating the visual benefits and inconveniences of stabilization.  It turns out that the filtering process of  the signal produced by the stabilization brings valuable intrinsic information about ego-motion.  This yields what we naturally called ego-motion scale space. Indeed the stabilization signal can be the object of a time-frequency analysis and yield an intrinsic description of the camera motion. 
  </p>

<p>
  24 Novembre 2016, 15h-16h, <a href="http://www-sop.inria.fr/members/Maureen.Clerc/index.php">Maureen Clerc</a> (INRIA)<br />
  <b>Title:</b> <i>Imaging brain activity</i>
  
    <b>[<a href="../slides/clerc.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> The living human brain is a tremendously complex organ that modern science is striving to better understand. Electroencephalography (EEG) allows to study it non-invasively, at a macroscopic scale. Typically, EEG datasets consist of multi-trial and multi-sensor signals, buried in very strong noise, making information extraction extremely challenging. In this talk I will address brain activity reconstruction and its application to real-time brain activity interpretation for brain-computer interfaces. 
  </p>

<p>
  3 Novembre 2016, 14h-15h, <a href="https://www.di.ens.fr/~mallat/">St√©phane Mallat</a> (Ecole Normale Superieure)<br />
  <b>Title:</b> <i>Unsupervised Learning and Inverse Problems with Deep Neural Networks</i>
  
    <b>[<a href="../slides/mallat.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> Deep neural networks have obtained remarkable results to learn generative image models. We show that it opens a new probabilistic framework to define non-Gaussian and non-ergodic random processes, which can be estimated with a reduced number of samples. The mathematics are introduced through multiscale wavelet scattering networks and applied to image and audio textures, but also to standard statistical physics processes such as Ising or stochatic geometry. We explain how such models are applied to inverse problems and super-resolution. 
  </p>

<p>
  3 Novembre 2016, 15h-16h, <a href="http://www-syscom.univ-mlv.fr/~chouzeno/">Emilie Chouzenoux</a> (Universit√© Paris-Est Marne-La-Vall√©e)<br />
  <b>Title:</b> <i>A Block Parallel Majorize-Minimize Memory Gradient Algorithm</i>
  
    <b>[<a href="../slides/chouzenoux.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> In the field of 3D image recovery, huge amounts of data need to be processed. Parallel optimization methods are then of main interest since they allow to overcome memory limitation issues, while benefiting from the intrinsic acceleration provided by recent multicore computing architectures. In this context, we propose a Block Parallel Majorize-Minimize Memory Gradient (BP3MG) algorithm for solving  large scale optimization problems. This algorithm combines a block coordinate strategy with an efficient parallel update. The proposed method is applied to a 3D microscopy image restoration problem involving a depth-variant blur, where it is shown to lead to significant computational time savings with respect to a sequential approach. 
  </p>

<p>
  6 Octobre 2016, 14h-15h, <a href="https://www.researchgate.net/profile/F_Champagnat">Fr√©d√©ric Champagnat</a> (ONERA)<br />
  <b>Title:</b> <i>R√©gularisation spatio-temporelle physique pour la mesure de champs de vitesse des fluides</i>
  
    <b>[<a href="../slides/champagnat.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> La v√©locim√©trie par imagerie de particules PIV est un outil essentiel d'investigation de la turbulence ouvrant la voie d'une analyse Lagrangienne et offrant un moyen d'acc√©der √† la mesure de pression. Le d√©veloppement de la PIV haute cadence (dite PIV TR pour 'time resolved') a permis l'√©mergence de nouvelles classes de m√©thodes reposant sur la coh√©rence spatio-temporelle des champs de vitesse. Les approches les plus courantes en PIV TR reposent sur un d√©veloppement de Taylor spatio-temporel du champ de mouvement. L'exploitation de ces r√©gularit√©s par des outils de r√©gularisation 'g√©n√©rique' permet d√©j√† de pallier efficacement les d√©fauts de l'imagerie TR (r√©solution spatiale limit√©e, biais li√©s au repliement spatial). L'objet de cette pr√©sentation est d'aborder la r√©gularisation physique de ces donn√©es qui s'appuie en l'esp√®ce sur les √©quations de Navier-Stokes incompressibles (ou des approximations physiques de ces derni√®res). Nous donnons d'abord les principes g√©n√©raux des m√©thodes d'assimilation qui permettent d'estimer des champs de vitesses respectant strictement Navier-Stokes √† partir d'images PIV TR. Puis nous pr√©sentons une alternative originale bas√©e sur une approximation de Navier-Stokes permettant sous certaines hypoth√®ses d'obtenir un champ r√©solu en temps √† partir d'une mesure du champ moyen et d'une mesure ponctuelle r√©solue en temps. Nous illustrons la capacit√© d'am√©lioration du RSB et de super-r√©solution de ces m√©thodes et tra√ßons leurs limites et les voies de recherche en cours. Collaborateurs: R. Yegavian, B. Leclaire, O. Marquet, S. Beneddine, D. Sipp 
  </p>

<p>
  6 Octobre 2016, 15h00-16h00, <a href="https://sites.google.com/site/stephanieallassonniere/">Stephanie Allassonniere</a> (Paris 5)<br />
  <b>Title:</b> <i>Mixed-effect model for the spatiotemporal analysis of longitudinal manifold-valued data</i>
  
    <b>[<a href="../slides/allassonniere.pdf">Slides</a>]</b>
  
  <br />
  <b>Abstract:</b> In this work, we propose a generic hierarchical spatiotemporal model for longitudinal manifold-valued data, which consist in repeated measurements over time for a group of individuals. This model allows us to estimate a group-average trajectory of progression, considered as a geodesic of a given Riemannian manifold. Individual trajectories of progression are obtained as random variations, which consist in parallel shifting and time reparametrization, of the average trajectory. These spatiotemporal transformations allow us to characterize changes in the direction and in the pace at which trajectories are followed. We propose to estimate the parameters of the model using a stochastic version of the expectation-maximization (EM) algorithm, the Monte Carlo Markov Chain Stochastic Approximation EM (MCMC SAEM) algorithm.  This generic spatiotemporal model is used to analyze the temporal progression of a family of biomarkers. This progression model estimates a normative scenario of the progressive impairments of several cognitive functions, considered here as biomarkers, during the course of Alzheimer‚Äôs disease. The estimated average trajectory provides a normative scenario of disease progression. Random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals. 
  </p>

:ET